!obj:pylearn2.train.Train {
    # MRI dataset.
    dataset: &data !obj:pl2mind.datasets.MRI.MRI_Standard {
        which_set: "train",
        center: &center False,
        variance_normalize: &vn False,
        unit_normalize: &un True,
        apply_mask: &app_mask True,
        dataset_name: &ds_name %(dataset_name)s
    },
    # Multi-layer perceptron model with 2 ReLU hidden layers followed by logistic regression.
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [ !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h0',
                     dim: %(nhid1)i,
                     sparse_init: 15
                 },  !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h1',
                     dim: %(nhid2)i,
                     sparse_init: 15
                 }, !obj:pylearn2.models.mlp.Softmax {
                     layer_name: 'y',
                     n_classes: 2,
                     irange: 0.
                 }
                ],
        nvis: %(nvis)d,
    },
    # Learning is done with stochastic gradient descent.
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: %(batch_size)i,
        learning_rate: %(learning_rate)f,
        train_iteration_mode: "even_shuffled_sequential",
        monitor_iteration_mode: "even_sequential",
        monitoring_batch_size: 5,
        monitoring_batches: 10,
        monitoring_dataset : 
            {
            'train' : *data,
            # Model is tested using a test dataset.  
            'test': !obj:pl2mind.datasets.MRI.MRI_Standard {
                           which_set: "test",
                           center: *center,
                           variance_normalize: *vn,
                           unit_normalize: *un,
                           apply_mask: *app_mask,
                           dataset_name: *ds_name
                           },
            },
        # Cost function is the default logistic regression with back propagation.
        cost: !obj:pylearn2.costs.mlp.Default {},
        # Momentum on gradients.
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
            init_momentum: %(init_momentum)f
        },
        # Termination criteria is provided when the yaml file is loaded.
        termination_criterion: %(termination_criterion)s,
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'test_y_misclass',
             save_path: "%(save_path)s_best.pkl"
        }, 
        # Momentum is increased from init to final
        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
            start: 1,
            saturate: 10,
            final_momentum: %(final_momentum)f
        }
    ],
    save_path: "%(save_path)s.pkl",
    save_freq: 10
}
